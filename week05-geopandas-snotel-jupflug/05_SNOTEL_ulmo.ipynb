{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowpocalypse '19\n",
    "February 10, 2019  \n",
    "David Shean\n",
    "\n",
    "## Objectives\n",
    "* Explore spatial relationships in time series of field observations\n",
    "* Learn about dynamic data fetching, ingestion into Pandas/GeoPandas and basic spatial analysis\n",
    "* Explore some simple interpolation routines\n",
    "* Explore some fundamental concepts and metrics for snow science\n",
    "\n",
    "## Read a bit about SNOTEL data for the Western U.S.\n",
    "\n",
    "https://www.wcc.nrcs.usda.gov/snow/\n",
    "\n",
    "This is actually a nice web interface, with some advanced querying and interactive visualization.  You can also download formatted ASCII files for later analysis.  This is great for one-time projects, but it's nice to have deterministic code that can be updated as new data appear, without manual steps.\n",
    "\n",
    "### About SNOTEL sites:\n",
    "* https://www.wcc.nrcs.usda.gov/about/mon_automate.html\n",
    "* https://www.wcc.nrcs.usda.gov/snotel/snotel_sensors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUAHSI WOF server and `ulmo` for Python queries\n",
    "\n",
    "### Acronym soup\n",
    "* SNOTEL = Snow Telemetry\n",
    "* CUAHSI = Consortium of Universities for the Advancement of Hydrologic Science, Inc\n",
    "* WOF = WaterOneFlow\n",
    "* WSDL = Web Services Description Language\n",
    "* USDA = United States Department of Agriculture\n",
    "* NRCS = National Resources Conservation Service\n",
    "* AWDB = Air-Water Database\n",
    "\n",
    "We are going to use a server set up by CUAHSI to serve the SNOTEL data, using a standardized database storage format and query structure.  You don't need to worry about this, but please quickly review the following:\n",
    "* http://hiscentral.cuahsi.org/pub_network.aspx?n=241 \n",
    "* http://his.cuahsi.org/wofws.html\n",
    "\n",
    "### Python options\n",
    "\n",
    "There are a few packages out there that offer convenience functions to query the online SNOTEL databases and unpack the results.  You can also write your own queries using the Python `requests` module and some built-in XML parsing libraries\n",
    "* climata (https://pypi.org/project/climata/) - last commit Sept 2017 (not a good sign)\n",
    "* ulmo (https://github.com/ulmo-dev/ulmo) - last commit a Feb 2019 (but not well maintained, and will be superseded soon)\n",
    "\n",
    "Hopefully not overwhelming amount of information - let's just go with ulmo for now.  I've done most of the work to prepare functions for querying and processing the data.  Once you wrap your head around all of the acronyms, it's pretty simple, basically running a few functions here: https://ulmo.readthedocs.io/en/latest/api.html#ulmo-cuahsi-wof\n",
    "\n",
    "I did discover a bug on the CUAHSI server side with the hourly SNOTEL data.  This works fine through climata and the https://wcc.sc.egov.usda.gov/awdbWebService/services?WSDL source), but I don't have time to redo everything with climata.  \n",
    "\n",
    "So we're just going to use ulmo with daily data for this exercise, but please feel free to experiment with my code, with climata, or parsing files you download via the web interface.\n",
    "\n",
    "Note that support for ulmo is ending, and will be superseded by another package called Quest (https://github.com/ulmo-dev/ulmo/issues/161)\n",
    "\n",
    "### Important ulmo installation note\n",
    "\n",
    "The current conda-forge build of ulmo requires an older version of pandas.  Also, some commits in the past few weeks fixed some bugs.  I've been told there will be a new conda-forge build next week, but we have geospatial data that needs analysis now!  So we're going to use a development version of ulmo, straight from the github source.  This is a good exercise, and will show you how to grab source code directly from github for local use and development.\n",
    "\n",
    "To install:\n",
    "1. Open a terminal on the Jupyterhub and `git clone https://github.com/ulmo-dev/ulmo.git` somewhere in your home directory\n",
    "2. `cd ulmo`\n",
    "3. `pip install -e .` (this will run a \"developer\" install, which means you can modify the source code and use updated versions in real-time)\n",
    "4. Restart the kernel in your notebook and you should be all set to `import ulmo`\n",
    "5. Note that if your Jupyterlab server restarts, you may need to repeat #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "%matplotlib inline\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import ulmo\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUAHSI WOF server information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://his.cuahsi.org/wofws.html\n",
    "wsdlurl = 'http://hydroportal.cuahsi.org/Snotel/cuahsi_1_1.asmx?WSDL'\n",
    "#wsdlurl = \"http://worldwater.byu.edu/interactive/snotel/services/index.php/cuahsi_1_1.asmx?WSDL\"  # WOF 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get information about all of the SNOTEL sites from the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ulmo.cuahsi.wof.get_sites(wsdlurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a moment to inspect the output\n",
    "\n",
    "* What is the `type`?\n",
    "* What happens when you print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store as a Pandas DataFrame\n",
    "* See the `from_dict` function\n",
    "* Use `orient` option so the sites comprise the DataFrame index, with columns for 'name', 'elevation_m', etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the DataFrame and prepare Point geometry objects\n",
    "* Convert 'location' column (contains dictionary with 'latitude' and 'longitude' values) to Shapely Points\n",
    "* Store as a new 'geometry' column\n",
    "* Drop the 'location' column, as this is no longer needed\n",
    "* Update the dtype of the 'elevation_m' column to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a GeoDataFrame with appropriate crs definition\n",
    "\n",
    "* Take a moment to familiarize yourself with the DataFrame structure and different fields.\n",
    "* Note that the index is a set of strings with format 'SNOTEL:1000_OR_SNTL'\n",
    "* Note the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scatterplot showing elevation values of all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove any records with incorrect coordinates of (0,0)\n",
    "* Note the updated number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Alaska (AK) points to isolate points over Western U.S.\n",
    "* Can use a spatial filter (see GeoPandas cx indexer functionality) or remove points where the site name contains 'AK'\n",
    "* Note the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update your scatterplot as sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an interactive plot with basemap using folium\n",
    "\n",
    "We haven't discussed as a class yet, but this is a simple, effective interactive visualization package (alternative to matplotlib)\n",
    "\n",
    "See the example here: https://python-visualization.github.io/folium/docs-v0.6.0/quickstart.html\n",
    "\n",
    "For your plot:\n",
    "* Create a map centered on the centroid of your filtered SNOTEL sites (remember GeoPandas `unary_union`)\n",
    "* Use the 'Stamen Terrain' basemap layer, experiment with `zoom_start` level\n",
    "* Export your GeoDataframe using `to_json()`, then load using `folium.features.GeoJson`\n",
    "* Add the points to the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit (if you have time after finishing the lab, and/or are interested in exploring folium)\n",
    "\n",
    "* Create an interactive `folium` map with a MarkerCluster that displays the SNOTEL site name and/or ID\n",
    "* This example is likely useful: https://ocefpaf.github.io/python4oceanographers/blog/2015/12/14/geopandas_folium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a histogram of SNOTEL site elevations\n",
    "* What is the highest SNOTEL site in the Western U.S.?  How about WA?\n",
    "* Do these elevations seem to provide a good sample of elevations where snow accumulates? (just think about this for a minute, I'm hoping we can revisit when we start working with rasters and DEM data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproject the sites GeoDataFrame to an equal-area projection\n",
    "* Can use the same projection from the GLAS example, or recompute based on bounds and center of SNOTEL sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scatterplot and overlay the state polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate WA sites and retrieve Snow Depth data\n",
    "* Could do a spatial join with WA polygon, as we did in the GLAS example\n",
    "* But probably easiest to filter records with 'WA' in the index\n",
    "    * Note: need to convert to `str`, then use `contains`\n",
    "* Sanity check - note number of records and create a quick scatterplot to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a histogram plot showing elevation of all SNOTEL sites in Western US and the WA sample\n",
    "* What do you notice about the WA sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the highest site in WA and assign to a variable named `sitecode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the server for information about this site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_info = ulmo.cuahsi.wof.get_site_info(wsdlurl, sitecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the returned information\n",
    "* Note available data series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_info['series'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's consider the 'SNOTEL:SNWD_D' variable (Daily Snow Depth)\n",
    "* Assign to a variable named `variablecode`\n",
    "* Get some information about the variable\n",
    "* Note the units, nodata value, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variablecode = 'SNOTEL:WTEQ_H'\n",
    "#variablecode = 'SNOTEL:WTEQ_D'\n",
    "#variablecode = 'SNOTEL:SNWD_H'\n",
    "variablecode = 'SNOTEL:SNWD_D'\n",
    "ulmo.cuahsi.wof.get_variable_info(wsdlurl, variablecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to fetch data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "def fetch(sitecode, variablecode='SNOTEL:SNWD_D', start_date='1950-10-01', end_date=today):\n",
    "    print(sitecode, variablecode, start_date, end_date)\n",
    "    values_df = None\n",
    "    try:\n",
    "        #Request data from the server\n",
    "        site_values = ulmo.cuahsi.wof.get_values(wsdlurl, sitecode, variablecode, start=start_date, end=end_date)\n",
    "        #Convert to a Pandas DataFrame   \n",
    "        values_df = pd.DataFrame.from_dict(site_values['values'])\n",
    "        #Parse the datetime values to Pandas Timestamp objects\n",
    "        values_df['datetime'] = pd.to_datetime(values_df['datetime'], utc=True)\n",
    "        #Set the DataFrame index to the Timestamps\n",
    "        values_df = values_df.set_index('datetime')\n",
    "        #Convert values to float and replace -9999 nodata values with NaN\n",
    "        values_df['value'] = pd.to_numeric(values_df['value']).replace(-9999, np.nan)\n",
    "        #Remove any records flagged with lower quality\n",
    "        values_df = values_df[values_df['quality_control_level_code'] == '1']\n",
    "    except:\n",
    "        print(\"Unable to fetch %s\" % variablecode)\n",
    "\n",
    "    return values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the full 'SNOTEL:SNWD_D' record for the highest station in WA without specifying start and end dates\n",
    "* Inspect the results\n",
    "* What are the first and last dates returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df = fetch(sitecode, variablecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a quick plot to view the time series\n",
    "* What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all daily snow depth records for all sites in WA\n",
    "* Note that this could take some time to run (10-20 min, depending on server load)\n",
    "* Loop through all sites names in the WA GeoDataFrame and fetch\n",
    "* Store in a dictionary\n",
    "* Convert the dictionary to a Pandas Dataframe\n",
    "\n",
    "## Save the DataFrame, so you don't have to download again\n",
    "* Can use a number of different formats, easiest to use a \"pickle\": https://www.pythoncentral.io/how-to-pickle-unpickle-tutorial/\n",
    "* Define a unique filename for the dataset (e.g., `pkl_fn='snotel_wa_snwd_d.pkl'`)\n",
    "* Write the DataFrame to disk\n",
    "* Read it to a temporary variable and verify that everything looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_fn = 'snotel_wa_snwd_d.pkl'\n",
    "#pkl_fn = variablecode.replace(':','_')+'_'+start_date+'_'+end_date+'.pkl'\n",
    "\n",
    "if os.path.exists(pkl_fn):\n",
    "    wa_dict = pd.read_pickle(pkl_fn)\n",
    "else:\n",
    "    #Define an empty dictionary to store returns for each site\n",
    "    value_dict = {}\n",
    "    for sitecode in sites_gdf_wa.index:\n",
    "        out = fetch(sitecode, variablecode, start_date, end_date)\n",
    "        if out is not None:\n",
    "            value_dict[sitecode] = out['value']\n",
    "    #Convert the dictionary to a DataFrame, automatically handles different datetime ranges (nice!)\n",
    "    wa_dict = pd.DataFrame.from_dict(value_dict)\n",
    "    #Write out\n",
    "    wa_dict.to_pickle(pkl_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the DataFrame\n",
    "* Note structure, number of timestamps, NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert snow depth inches to cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute daily snow accumulation at all stations\n",
    "* See the `diff` function\n",
    "* Make sure you specify the axis properly to difference over time (not station to station), and sanity check\n",
    "* Create a plot of differences for all sites\n",
    "    * Adjusting the ylim appropriately\n",
    "    * Probably best to turn off the legend in your plot call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How might you filter these differences to remove artifacts and outliers?\n",
    "* If a single station shows an increase of 2 m, but all others show a decrease, is that realistic?\n",
    "* Create a plot showing the median difference values across all stations for each day\n",
    "* Do you think you can confidently identify large snow accumulation events using these difference values?\n",
    "* Could likely design a filter to remove outliers using mean +/- (3 * std) for all stations\n",
    "    * Maybe come back to this if you have time later, for now, just note the presence of measurement errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the count of valid records (not NaN) available for each station\n",
    "* Create a bar plot, sorted from shortest to longest record\n",
    "* What is the longest record available for SNWD_D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the count dataseries as a new column in the WA sites GeoDataframe\n",
    "* Should be easy, let Pandas do the work to match site index values\n",
    "* Verify your site DataFrame now has a count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column for the long-term median snow depth at each site\n",
    "* Note: to do this properly, probably want to remove any values near 0 before computing the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column for the long-term max snow depth on record for each site\n",
    "* Might need to be careful about measurement errors here - maybe look at values and filter obvious outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column for the daily difference (snow accumulation) at all sites for the recent Feb 8 snow event\n",
    "* Note, you may need to use `mydataframe.loc[pd.Timestamp('YYYY-MM-DD')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_wa['2019-02-05_accum'] = wa_dict.diff(axis=0).loc[pd.Timestamp('2019-02-05')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a column for the snow depth on the most recent day in the record\n",
    "* Use the last index value (should be today, or whenever you ran the query)\n",
    "* Note that the index is not a string, it is a Pandas Timestamp object: `pd.Timestamp('2019-02-06 00:00:00')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = wa_dict.index[-1]\n",
    "print(ts)\n",
    "print(type(ts))\n",
    "#sites_gdf_wa[ts] = wa_dict.iloc[-1]\n",
    "sites_gdf_wa['2019-02-06'] = wa_dict.iloc[-1]\n",
    "sites_gdf_wa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some scatterplots to review these metrics\n",
    "* Need to remove NaN records on the fly before plotting - see the `dropna()` method\n",
    "    * Best to apply after isolating the column you want to plot, consider `mydataframe[['median', 'geometry]].dropna().plot(column='median', ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and plot at least one additional metric of interest for snow depth at the WA sites\n",
    "* Consider aggregation over time and/or space\n",
    "* Do some brief analysis and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you identify the SNOTEL site that received the most snow accumulation during the Feb 2019 storm(s)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snow depth vs. elevation analysis\n",
    "* Let's look at snow depth from most recent day in the record\n",
    "* Create a quick scatterplot of elevation vs. snow depth for all sites on this day (or several days if desired)\n",
    "* Do you see a relationship?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate recent accumulation pattern for the full WA state domain\n",
    "* Note that you wouldn't want to do this in practice for such a sparse sample, but let's explore for purposes of illustration\n",
    "* We'll use the `scipy.interpolate.griddata` function here, using 'nearest' to start\n",
    "* Try a few different interpolation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column name of variable to interpolate\n",
    "var = '2019-02-08_accum'\n",
    "#Extract the column and geometry, drop NaNs\n",
    "sites_gdf_wa_dropna = sites_gdf_wa[[var,'geometry']].dropna()\n",
    "#Pull out (x,y,val)\n",
    "x = sites_gdf_wa_dropna.geometry.x\n",
    "y = sites_gdf_wa_dropna.geometry.y\n",
    "z = sites_gdf_wa_dropna[var]\n",
    "#Get min and max values\n",
    "zlim = (z.min(), z.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the spatial extent of the points - we will interpolate across this domain\n",
    "bounds = sites_gdf_wa_dropna.total_bounds\n",
    "#Spatial interpolation step of 1 km\n",
    "dx,dy = (1000,1000)\n",
    "#Create 1D arrays of grid cell coordinates in the x and y directions\n",
    "xi = np.arange(np.floor(bounds[0]), np.ceil(bounds[2]),dx)\n",
    "yi = np.arange(np.floor(bounds[1]), np.ceil(bounds[3]),dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2D grids from the xi and yi grid cell coordinates\n",
    "xx, yy = np.meshgrid(xi, yi)\n",
    "#Interpolate values\n",
    "zi = scipy.interpolate.griddata((x,y), z, (xx, yy), method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a plot\n",
    "f, ax = plt.subplots()\n",
    "#Define extent of the interpolated grid in projected coordinate system, using matplotlib extent format (left, right, bottom, top)\n",
    "mpl_extent = [bounds[0], bounds[2], bounds[1], bounds[3]]\n",
    "#Plot the interpolated grid, providing known extent\n",
    "#Note: need the [::-1] to flip the grid in the y direction\n",
    "ax.imshow(zi[::-1,], cmap='inferno', extent=mpl_extent, clim=zlim)\n",
    "#Overlay the original point values with the same color ramp\n",
    "sites_gdf_wa.plot(ax=ax, column=var, cmap='inferno', markersize=20, edgecolor='w', vmin=zlim[0], vmax=zlim[1], legend=True)\n",
    "#Make sure aspect is equal\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra credit (or, some additional ideas to explore)\n",
    "\n",
    "If you have some time (or curiosity), feel free to explore some of these, or define your own questions.\n",
    "\n",
    "1. Compute snow depth statistics across all sites grouping by Water Year\n",
    "2. Long-term median snowdepth for each day of the year, how does 2019 compare (percent of long-term median)\n",
    "4. Identify date of first major snow accumulation event each year, date of max snow depth, date of snow disappearance - any evolution over time?\n",
    "5. Split sites into elevation bands and analyze various metrics\n",
    "6. Explore other interpolation methods for sparse data\n",
    "7. Create an animated map of daily accumulation in WA for the past two weeks\n",
    "8. Look at other variables for the SNOTEL sites (WTEQ_D, SNWD_H)\n",
    "    * Note that WTEQ_D time series begin much earlier than SNWD_D\n",
    "9. Rerun for the full Western U.S."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
